import os
from dotenv import load_dotenv

# Carregar variÃ¡veis de ambiente
load_dotenv()

# ConfiguraÃ§Ãµes do Banco de Dados
DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'port': os.getenv('DB_PORT', '5432'),
    'database': os.getenv('DB_NAME', 'itributos'),
    'user': os.getenv('DB_USER', 'postgres'),
    'password': os.getenv('DB_PASSWORD', 'postgres')
}

# Ollama Configuration
OLLAMA_HOST = os.getenv('OLLAMA_HOST', 'http://localhost:11434')
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'qwen2.5:3b')

# Google Gemini Configuration
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
GEMINI_MODEL = os.getenv('GEMINI_MODEL', 'gemini-2.0-flash-exp')

# Lista de modelos Gemini para fallback (em ordem de preferÃªncia)
# Quando um modelo atingir o limite, tenta o prÃ³ximo automaticamente
GEMINI_FALLBACK_MODELS = [
    'gemini-3-flash-preview',                   # Modelo experimental mais novo
    'gemini-2.0-flash-exp',                     # Mais rÃ¡pido, mais requisiÃ§Ãµes gratuitas
    'gemini-2.5-flash',                         # Modelo mais novo, bom equilÃ­brio
    'gemini-2.5-pro',                           # Mais capaz, mas mais lento e com menos requisiÃ§Ãµes gratuitas
    'gemini-2.5-flash-preview-09-2025',         # Ãšltimo modelo disponÃ­vel
    'gemini-2.5-flash-lite',                    # VersÃ£o leve do 2.5
    'gemini-2.5-flash-lite-preview-09-2025',    # VersÃ£o leve do 2.5 preview
    'gemini-2.0-flash',                         # Modelo estÃ¡vel, mas mais antigo
    'gemini-2.0-flash-lite',                    # VersÃ£o leve do 2.0
    'gemini-1.5-flash',                         # RÃ¡pido, bom para tarefas simples
    'gemini-1.5-pro',                           # Mais capaz, limite menor
]

# Configurar qual modelo inicial usar (se GEMINI_MODEL nÃ£o estiver configurado)
if GEMINI_MODEL not in GEMINI_FALLBACK_MODELS:
    GEMINI_FALLBACK_MODELS.insert(0, GEMINI_MODEL)

# ConfiguraÃ§Ãµes de LLM
LLM_PROVIDER = os.getenv('LLM_PROVIDER', 'gemini')

# ConfiguraÃ§Ãµes do Cache
CACHE_DIR = os.getenv('CACHE_DIR', './cache')
CACHE_TTL_SECONDS = int(os.getenv('CACHE_TTL_SECONDS', '3600'))

# ConfiguraÃ§Ãµes do Streamlit
STREAMLIT_PORT = int(os.getenv('STREAMLIT_PORT', '8501'))

# ValidaÃ§Ã£o de configuraÃ§Ã£o
if LLM_PROVIDER == 'gemini' and not GEMINI_API_KEY:
    print("âš ï¸ AVISO: GEMINI_API_KEY nÃ£o configurada!")
    print("Configure no arquivo .env para usar Gemini")
    print("Obtenha sua chave em: https://aistudio.google.com/app/apikey")
else:
    print(f"âœ… LLM Provider: {LLM_PROVIDER}")
    print(f"ğŸ“¦ Ollama: {OLLAMA_HOST} ({OLLAMA_MODEL})")
    if GEMINI_API_KEY:
        print(f"ğŸ”‘ Gemini configurado")
        print(f"ğŸ¯ Modelo principal: {GEMINI_MODEL}")
        print(f"ğŸ”„ Fallback habilitado: {', '.join(GEMINI_FALLBACK_MODELS[1:] if len(GEMINI_FALLBACK_MODELS) > 1 else ['Nenhum'])}")
